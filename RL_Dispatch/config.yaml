# # giving case generation infomation
# TrainLoader:
#   base_grid:
#     existed_in_pp: false
#     name: case14
#     grid_path: './data/base_grid/14nodes/node14.txt'

#   sample_construct:
#     # generate multiple initial power flow cases
#     - !caseDuplicator
#       input: '__base_grid__'
#       case_num: 2
#       output:
#         -
#           name: 'origin_cases'
#           fields: '__all__'
#     - !pfCaseInitializer
#       input: 'origin_cases'
#       case_transform:
#         -
#           name: fluctuation_abs
#           fluc_states: ['load:p_mw', 'load:q_mvar']
#           fluc_vals: [[-5. ,5.], [-2., 2.]]
#         -
#           name: fluctuation_rel
#           fluc_states: ['load:p_mw', 'load:q_mvar']
#           fluc_ratios: [[0.6, 1.2], [0.6, 1.2]]
#       output:
#         -
#           name: 'origin_cases_with_fluc'
#           fields: '__all__'
#     - !pfSolver
#       input: 'origin_cases_with_fluc'
#       output:
#         -
#           name: 'power_flow_cases_with_fluc'
#           fields: '__all__'

#     - !pfActionGenerator
#       input: 'origin_cases_with_fluc'
#       action_state: ['gen:vm_pu']
#       action_type: ['discrete']
#       action_enum: [[0.9, 0.95, 1.0, 1.05, 1.1]]
#       output:
#         -
#           name: 'gen_voltage_mag_action'
#           fields: '__all__'

#     - !pfActionExecutor
#       input: ['origin_cases_with_fluc', 'gen_voltage_mag_action']
#       output:
#         -
#           name: 'cases_after_'
#     #- !powerFlowSolver
#     #  input: 'origin_cases_with_fluc'
#     #  output:
#     #    -
#     #      name: 'power_flow_cases'
#     #      fields: '__all__'

#     #- !rewardCalculator
#       #

#   input_def:
#     fields: ['init_state', 'action', 'reward', 'next_state']

# case generation
base_grid: "packages/grid/14nodes/14nodes.json"
num_case: 100
obj2change: ["load"]
attr2change: ["p_mw"]
fluc_area: [[0.8, 1.2]]


# experiment
total_step: 50000
data_folder: "packages/data/14nodes"
# replay_buffer_size: 1000000
replay_buffer_size: 10000
# learning_starts: 50000
learning_starts: 500
learning_freq: 4
target_update_freq: 500
# log
log_every_n_steps: 500


# wrapper
# reward_border: [0.85, 0.95, 1.05, 1.15]
reward_border: [0.94, 0.97, 1.03, 1.06]
reward_value: [-1, 0.5, 1]
diverge_border: [0.4, 2]
# action
num_actor: 4
actor: gen
action_attribute: vm_pu
action_enum: [0.95, 0.975, 1, 1.025, 1.05]
# observation
observer: [res_bus, res_bus, res_line, res_line]
observe_attribute: [vm_pu, va_degree, p_from_mw, q_from_mvar]
target: [res_bus]
target_attribute: [vm_pu]
# episode
max_step: 4


# schedule
schedule_type: linear
schedule_timesteps: 10000
final_p: 0.1


# Hyper parameter
# network
num_layer: 3
layer_size: [128, 64, 32]
# Deep learning training
batch_size: 32
gamma: 0.99
learning_rate: 0.00025
alpha: 0.95
eps: 0.01


