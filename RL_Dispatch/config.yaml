# case generation
# base_grid: "packages/grid/118nodes/118nodes.json"
base_grid: "packages/grid/14nodes/14nodes.json"
num_train_case: 400
num_test_case: 100
obj2change: ["load"]
attr2change: ["p_mw"]
# fluc_area: [[[0.6, 0.8],[1.2, 1.4]]]
fluc_area: [[[0.6, 1.1],[0.6, 1.1]]]


# experiment
total_step: 2000
# data_folder: "packages/data/118nodes"
train_folder: "packages/data/14nodes_train"
# test_folder: "packages/data/14nodes_test"
# replay_buffer_size: 1000000
replay_buffer_size: 10000
# learning_starts: 50000
learning_starts: 500
learning_freq: 4
target_update_freq: 500
# log
log_every_n_steps: 500
do_test: False


# problem formulation
target: ["res_bus"]
target_attribute: ["vm_pu"]
# reward_border: [
#                 [0.9, 0.95], 
#                 [0.95, 1.05], 
#                 [1.05, 1.1], 
#               ]
reward_border: [
                [0.9, 0.97], 
                [0.97, 1.03], 
                [1.03, 1.1], 
              ]
# reward_list: [-0.5, 0.5, 1, 0.5, -0.5]
reward_list: [-0.5, 1, -0.5]
reward_worst: -1
diverge_border: [0.4, 2]


# action
# num_actor: 4
actor: ["gen"]
action_attribute: ["vm_pu"]
action_enum: [0.95, 0.975, 1, 1.025, 1.05]

# observation
observer: [res_bus, res_bus, res_line, res_line]
observe_attribute: [vm_pu, va_degree, p_from_mw, q_from_mvar]

# episode
max_step: 5


# schedule
schedule_type: linear
schedule_timesteps: 1000
# schedule_timesteps: 1
final_p: 0.01
# final_p: 0

# learning rate
optim_type: RMSprop
learning_rate: 0.01
step_optimizer: False
step_size: 10000
# learning rate decay
gamma_lr: 0.1

# Hyper parameter
# network
num_layer: 3
layer_size: [128, 64, 32]
use_batchnorm: True
# Deep learning training
batch_size: 32
# next Q decay
gamma: 0.99
alpha: 0.95
eps: 0.01
dropout: 0.1
